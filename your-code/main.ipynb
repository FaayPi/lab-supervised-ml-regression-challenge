{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Supervised ML Regression Competition</center></h1>\n",
    "\n",
    "\n",
    "<img align=\"center\" src=\"https://compraracciones.com/wp-content/uploads/2021/04/insurance.jpg\" style=\"height:200px\" style=\"width:100px\"/>\n",
    "\n",
    "<hr style=\"border:2px solid pink\"> </hr>\n",
    "\n",
    "You have been assigned the task of building a model that will predict the insurance cost\n",
    "\n",
    "You'll find the data in the csv file `insurance`\n",
    "\n",
    "\n",
    "- target col: \"charges\"\n",
    "\n",
    "\n",
    "<hr style=\"border:2px solid pink\"> </hr>\n",
    "\n",
    "\n",
    "**Guidelines:** \n",
    "\n",
    "\n",
    "- train_test_split\n",
    "    - random state = 42\n",
    "    - test size = 0.3\n",
    "\n",
    "\n",
    "- The one who gets the highest r2-score on test data wins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Data Exploration\n",
    "\n",
    "Let's start by loading our dataset and taking a first look at it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "insurance = pd.read_csv(\"insurance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking for Missing Values\n",
    "\n",
    "It's important to know if our data has any missing values. Let's check that next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Descriptive Statistics\n",
    "\n",
    "Now, let's move on to some descriptive statistics.\n",
    "\n",
    "Understanding the distribution of our data is crucial. Let's calculate some descriptive statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Analysis\n",
    "\n",
    "Visualizing the distributions of our features can provide valuable insights. Let's plot the distributions for 'age', 'bmi', and 'charges'.\n",
    "\n",
    "### Task:\n",
    "- Plot the histogram for 'age'\n",
    "- Plot the histogram for 'bmi'\n",
    "- Plot the histogram for 'charges'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Angenommen, df ist dein DataFrame mit numerischen Spalten\n",
    "\n",
    "insurance.hist(bins=30, figsize=(12, 8))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Relationship Between Variables\n",
    "\n",
    "Let's explore the relationship between some of our features and the target variable 'charges'. We'll create scatter plots to visualize these relationships.\n",
    "\n",
    "### Task:\n",
    "- Create a scatter plot for 'age' vs 'charges'\n",
    "- Create a scatter plot for 'bmi' vs 'charges'\n",
    "- Create a scatter plot for 'children' vs 'charges'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(insurance['age'], insurance['charges'])\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Charges')\n",
    "plt.title('Scatterplot Age and Charges')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(insurance['bmi'], insurance['charges'])\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Charges')\n",
    "plt.title('Scatterplot BMI and Charges')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(insurance['children'], insurance['charges'])\n",
    "plt.xlabel('Children')\n",
    "plt.ylabel('Charges')\n",
    "plt.title('Scatterplot Children and Charges')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Categorical Analysis\n",
    "\n",
    "Let's analyze the categorical features 'sex', 'smoker', and 'region' to see how they relate to 'charges'.\n",
    "\n",
    "### Task:\n",
    "- Plot the distribution of 'charges' for different 'sex'\n",
    "- Plot the distribution of 'charges' for different 'smoker'\n",
    "- Plot the distribution of 'charges' for different 'region'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution von 'charges' nach 'sex'\n",
    "sns.boxplot(x='sex', y='charges', data=insurance)\n",
    "plt.title('Charges distribution by sex')\n",
    "plt.show()\n",
    "\n",
    "# Distribution von 'charges' nach 'smoker'\n",
    "sns.boxplot(x='smoker', y='charges', data=insurance)\n",
    "plt.title('Charges distribution by smoker')\n",
    "plt.show()\n",
    "\n",
    "# Distribution von 'charges' nach 'region'\n",
    "sns.boxplot(x='region', y='charges', data=insurance)\n",
    "plt.title('Charges distribution by region')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis\n",
    "\n",
    "To understand how our numerical features relate to each other and to the target variable, let's calculate and visualize the correlation matrix.\n",
    "\n",
    "### Task:\n",
    "- Calculate the correlation matrix for the dataset\n",
    "- Visualize the correlation matrix using a heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = insurance[[\"age\", \"bmi\", \"children\", \"charges\"]].corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find the Naive Baseline\n",
    "\n",
    "Before we build any models, let's establish a naive baseline. This will help us understand how well our models perform compared to a simple approach. In regression problems, the naive baseline is often the mean of the target variable.\n",
    "\n",
    "### Task:\n",
    "- Calculate the mean of the target variable 'charges'\n",
    "- Explain why it's important to establish a naive baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_baseline = insurance[\"charges\"].mean()\n",
    "naive_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Modelling Without GridSearch or Pipeline\n",
    "\n",
    "Let's build a simple linear regression model without any feature engineering, grid search, or pipeline. This will serve as our initial baseline for comparison.\n",
    "\n",
    "### Task:\n",
    "- Split the data into training and test sets\n",
    "- Train a simple linear regression model\n",
    "- Evaluate its performance using regression metrics\n",
    "- Write it down as a markdown below so you can keep track. This is a scientific experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "target = insurance[\"charges\"]\n",
    "features = insurance.drop(columns=[\"charges\"])\n",
    "features_encoded = pd.get_dummies(features, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_encoded, target, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "\n",
    "# R² Score (Bestimmtheitsmaß)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Now, let's brainstorm and create some new features to see if we can improve the model's performance.\n",
    "\n",
    "### Questions:\n",
    "1. Should we create an interaction feature between 'bmi' and 'children'? \n",
    "2. Should we create age groups to see if the model improves by categorizing age?\n",
    "3. Should we create a high-risk indicator based on 'smoker' and 'bmi'?\n",
    "\n",
    "- Remember nothing is set in stone, this is your experiment, your hypothesis. You may not need to, but its important to explore these questions\n",
    "\n",
    "### Task:\n",
    "- Create new features based on the questions above\n",
    "- Explain the rationale behind each feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction feature bmi and children\n",
    "bmi_children = insurance[\"bmi\"]*insurance[\"children\"]\n",
    "\n",
    "# age groups\n",
    "bins = [0, 18, 30, 45, 60, 100]  # Altersschnittpunkte\n",
    "labels = ['0-17', '18-29', '30-44', '45-59', '60+']  # Labels für die Gruppen\n",
    "insurance['age_group'] = pd.cut(insurance['age'], bins=bins, labels=labels, right=False)\n",
    "print(insurance[['age', 'age_group']].head())\n",
    "\n",
    "# high risk indicator for smoker and high bmi\n",
    "high_bmi_threshold = 30\n",
    "insurance['high_risk'] = ((insurance['smoker'] == 'yes') & (insurance['bmi'] > high_bmi_threshold)).astype(int)\n",
    "print(insurance[['smoker', 'bmi', 'high_risk']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling with Feature Engineering\n",
    "\n",
    "Now that we have new features, let's see if they improve our model's performance.\n",
    "Did it improve the performance? Yes? No? Why\n",
    "\n",
    "### Task:\n",
    "- Split the data into training and test sets\n",
    "- Train a linear regression model with the new features\n",
    "- Evaluate its performance using regression metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "target = insurance[\"charges\"]\n",
    "features = insurance.drop(columns=[\"charges\"])\n",
    "features_encoded = pd.get_dummies(features, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_encoded, target, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "\n",
    "# R² Score (Bestimmtheitsmaß)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelling with Pipeline and Grid Search\n",
    "\n",
    "Now, let's see how using pipelines can simplify our workflow and prevent data leakage. We'll also use GridSearchCV to find the best hyperparameters.\n",
    "\n",
    "### Task:\n",
    "- Create a pipeline that includes scaling and linear regression\n",
    "- Define a parameter grid for hyperparameter tuning\n",
    "- Use GridSearchCV to find the best parameters and evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# pipeline including scaling and linear regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),   \n",
    "    ('model', Ridge())    \n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'model__alpha': range(0, 1001)\n",
    "}\n",
    "\n",
    "# initialize GridSearch\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal Parameter:\", grid.best_params_)\n",
    "print(\"Best CV-result:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trying Another Model with Pipeline\n",
    "\n",
    "Let's try using a Gradient Boosting Regressor to see if it performs better.\n",
    "\n",
    "### Task:\n",
    "- Create and use a pipeline for Gradient Boosting Regressor\n",
    "- Define a parameter grid for grid search\n",
    "- Use GridSearchCV to find the best parameters and evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameter: {'model__learning_rate': 0.05, 'model__max_depth': 3, 'model__n_estimators': 100}\n",
      "Best CV-result: 0.8538321530350546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# pipeline including scaling and linear regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),   \n",
    "    ('model', GradientBoostingRegressor(random_state=42))    \n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [100, 200, 300],     # Anzahl Bäume\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1],  # Lernrate\n",
    "    \"model__max_depth\": [3, 4, 5]               # Baumtiefe\n",
    "}\n",
    "\n",
    "# initialize GridSearch\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal Parameter:\", grid.best_params_)\n",
    "print(\"Best CV-result:\", grid.best_score_)\n",
    "\n",
    "y_pred_final = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GridSearch with Several Models\n",
    "\n",
    "Finally, let's compare several models using GridSearchCV to find the best one.\n",
    "\n",
    "### Task:\n",
    "- Define multiple models and their parameter grids\n",
    "- Use GridSearchCV to find the best model and parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# LASSO\n",
    "# pipeline including scaling and linear regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),   \n",
    "    ('model', Lasso(random_state=42))    \n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    \"model__alpha\": range(0,1001)\n",
    "}\n",
    "\n",
    "# initialize GridSearch\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal Parameter:\", grid.best_params_)\n",
    "print(\"Best CV-result:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Decision Tree Regressor\n",
    "\n",
    "# pipeline including scaling and linear regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),   \n",
    "    ('model', DecisionTreeRegressor(random_state=42))    \n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_leaf_nodes': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# initialize GridSearch\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal Parameter:\", grid.best_params_)\n",
    "print(\"Best CV-result:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best performing linear regression model is GradientBoosterRegressor with R2 of 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Master Challenge\n",
    "\n",
    "## 8. Calculating Potential Cost or Loss\n",
    "\n",
    "### Challenge:\n",
    "Now that you've built and optimized your models, it's time for the final challenge! Your task is to minimize the Root Mean Squared Error (RMSE) of your model's predictions and calculate the potential financial impact of your model's errors.\n",
    "\n",
    "### Task:\n",
    "1. Calculate the RMSE of your final model's predictions.\n",
    "2. Break down the errors into underestimation and overestimation.\n",
    "3. Calculate the total potential cost or loss to the company.\n",
    "4. Compete with your classmates to see who can achieve the lowest RMSE and financial impact!\n",
    "\n",
    "### Explanation:\n",
    "The RMSE provides an estimate of the average error in your model's predictions. We will also analyze the errors by categorizing them into underestimations and overestimations to understand their financial impact.\n",
    "\n",
    "#### Steps to Calculate Underestimation and Overestimation Errors:\n",
    "\n",
    "1. **Calculate RMSE**:\n",
    "   - Use the `mean_squared_error` function from `sklearn.metrics` and pass your actual values (`y_test`) and predicted values (`y_pred_final`) to it.\n",
    "   - Take the square root of the result to get the RMSE.\n",
    "   \n",
    "2. **Calculate Underestimation Error**:\n",
    "   - Identify the instances where the actual charges (`y_test`) are greater than the predicted charges (`y_pred_final`).\n",
    "   - For these instances, calculate the difference between the actual and predicted charges.\n",
    "   - Sum these differences to get the total underestimation error.\n",
    "\n",
    "3. **Calculate Overestimation Error**:\n",
    "   - Identify the instances where the actual charges (`y_test`) are less than the predicted charges (`y_pred_final`).\n",
    "   - For these instances, calculate the difference between the predicted and actual charges.\n",
    "   - Sum these differences to get the total overestimation error.\n",
    "\n",
    "4. **Calculate Total Potential Cost or Loss**:\n",
    "   - Add the total underestimation error and the total overestimation error to get the total potential cost or loss.\n",
    "\n",
    "### Let's see who can build the best model!\n",
    "\n",
    "#### Detailed Instructions:\n",
    "\n",
    "1. **Calculate RMSE**:\n",
    "   - Use `mean_squared_error` with `y_test` and `y_pred_final`.\n",
    "   - Use `np.sqrt` to take the square root of the result.\n",
    "\n",
    "2. **Calculate Underestimation Error**:\n",
    "   - Use a boolean condition to filter `y_test` values that are greater than `y_pred_final`.\n",
    "   - Subtract the predicted values from the actual values for these instances.\n",
    "   - Sum these differences.\n",
    "\n",
    "3. **Calculate Overestimation Error**:\n",
    "   - Use a boolean condition to filter `y_test` values that are less than `y_pred_final`.\n",
    "   - Subtract the actual values from the predicted values for these instances.\n",
    "   - Sum these differences.\n",
    "\n",
    "4. **Calculate Total Potential Cost or Loss**:\n",
    "   - Add the results of the underestimation error and overestimation error to get the total potential cost or loss.\n",
    "\n",
    "### Example Walkthrough:\n",
    "\n",
    "1. **Calculate RMSE**:\n",
    "   - `rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))`\n",
    "   - This gives you the average prediction error in dollars.\n",
    "\n",
    "2. **Calculate Underestimation Error**:\n",
    "   - `underestimation_error = np.sum(y_test[y_test > y_pred_final] - y_pred_final[y_test > y_pred_final])`\n",
    "   - This gives you the total amount by which the model undercharged.\n",
    "\n",
    "3. **Calculate Overestimation Error**:\n",
    "   - `overestimation_error = np.sum(y_pred_final[y_test < y_pred_final] - y_test[y_test < y_pred_final])`\n",
    "   - This gives you the total amount by which the model overcharged.\n",
    "\n",
    "4. **Calculate Total Potential Cost or Loss**:\n",
    "   - `total_potential_loss = underestimation_error + overestimation_error`\n",
    "   - This gives you the total financial impact of the model's errors.\n",
    "\n",
    "### Leaderboard:\n",
    "Post your RMSE score and total potential cost or loss on the class leaderboard. The student with the lowest RMSE and total potential cost or loss wins bragging rights\n",
    "\n",
    "### Post Your Results \n",
    "\n",
    "- Name\n",
    "- Model Type\n",
    "- RMSE\n",
    "- Underestimation Error\n",
    "- Overestimation Error\n",
    "- Total Potential Cost/Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 5220.14\n",
      "undererstimation error: -445172.67\n",
      "overerstimation error: 562864.53\n",
      "total potential loss: 117691.86\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "import numpy as np\n",
    "rmse_final_model = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "\n",
    "# Underestimation Error\n",
    "underestimation_error = np.sum(y_pred_final[y_test > y_pred_final] - y_test[y_test > y_pred_final])\n",
    "print(f\"undererstimation error: {underestimation_error:.2f}\")\n",
    "\n",
    "# Overestimation Error\n",
    "overestimation_error = np.sum(y_pred_final[y_test < y_pred_final] - y_test[y_test < y_pred_final])\n",
    "print(f\"overerstimation error: {overestimation_error:.2f}\")\n",
    "\n",
    "# Total Potential Cost or Loss\n",
    "total_potential_loss = underestimation_error + overestimation_error\n",
    "\n",
    "print(f\"total potential loss: {total_potential_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've completed the lab. Here's a summary of what we've covered:\n",
    "1. Established a naive baseline using the mean of the target variable.\n",
    "2. Built an initial linear regression model without any feature engineering or optimization.\n",
    "3. Performed feature engineering to create new, potentially useful features.\n",
    "4. Used pipelines and GridSearchCV to optimize the model.\n",
    "5. Evaluated the final model's performance using RMSE to understand its business impact.\n",
    "\n",
    "By following these steps, you now have a robust understanding of how to approach a regression problem, from initial exploration to model optimization and business impact assessment. Great job!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
